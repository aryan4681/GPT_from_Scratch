{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzGgOAdNJlc-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "collapsed": true,
        "id": "EO-aV2qJJw8F",
        "outputId": "579a8b49-ac31-47bd-904a-c848477468d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d83a3708-c444-4604-9f9b-0b78a14b8782\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d83a3708-c444-4604-9f9b-0b78a14b8782\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving shakespeare.txt to shakespeare.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"shakespeare.txt\", \"r\") as file:\n",
        "    content = file.read()\n",
        "    print(content[:30])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DS_2gc56Lsl8",
        "outputId": "75951f38-f5bf-4821-9c0b-8598a4fb3198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Project Gutenberg eBook of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_chars= set(content)"
      ],
      "metadata": {
        "id": "N31nQ30BPc8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(unique_chars)\n",
        "\n",
        "print(len(unique_chars))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QqioM4E5SjK1",
        "outputId": "ca5c56b1-dd36-4caf-b43e-9f2bf7c37748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'-', '0', 'c', '’', 'C', 'y', 'Æ', 'v', 'I', 'D', 'd', ',', 'E', 'l', 'G', 'N', '4', 'œ', 'V', 'n', '6', '3', '‘', 'Z', 'U', 'q', 'À', '\\n', \"'\", 'X', 'K', 'P', '\\ufeff', ':', '?', 'O', '…', '”', 'ë', '7', 'r', 'L', 'm', '\\t', 'x', 'æ', 'Y', 'M', 'b', '$', '—', 'H', 'É', 'B', '.', 'e', 'à', '_', 'g', 'a', '9', '1', ' ', 'u', '/', '“', 'z', '#', '8', 'Q', '!', 'A', 'é', '*', 'è', 'j', 'k', 's', 'î', '(', '2', 'p', 'h', 'w', 'J', '™', 'F', 'Ç', 't', '5', 'ç', 'ê', '&', 'o', ')', 'T', 'f', 'R', '•', 'i', 'S', 'â', 'W', '%', '[', ';', ']'}\n",
            "107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_chars = list(unique_chars)\n",
        "ordered_unique_chars = sorted(unique_chars)\n",
        "print(ordered_unique_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wcETiEwlSlTY",
        "outputId": "731b348a-2fdc-41af-bef4-205881114a25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\t', '\\n', ' ', '!', '#', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'À', 'Æ', 'Ç', 'É', 'à', 'â', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'î', 'œ', '—', '‘', '’', '“', '”', '•', '…', '™', '\\ufeff']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_id = {char: idx for idx, char in enumerate(ordered_unique_chars)}\n",
        "print(char_to_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ_W78PsTH8R",
        "outputId": "c0728795-db4b-48b2-9f00-6deccefada8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\\t': 0, '\\n': 1, ' ': 2, '!': 3, '#': 4, '$': 5, '%': 6, '&': 7, \"'\": 8, '(': 9, ')': 10, '*': 11, ',': 12, '-': 13, '.': 14, '/': 15, '0': 16, '1': 17, '2': 18, '3': 19, '4': 20, '5': 21, '6': 22, '7': 23, '8': 24, '9': 25, ':': 26, ';': 27, '?': 28, 'A': 29, 'B': 30, 'C': 31, 'D': 32, 'E': 33, 'F': 34, 'G': 35, 'H': 36, 'I': 37, 'J': 38, 'K': 39, 'L': 40, 'M': 41, 'N': 42, 'O': 43, 'P': 44, 'Q': 45, 'R': 46, 'S': 47, 'T': 48, 'U': 49, 'V': 50, 'W': 51, 'X': 52, 'Y': 53, 'Z': 54, '[': 55, ']': 56, '_': 57, 'a': 58, 'b': 59, 'c': 60, 'd': 61, 'e': 62, 'f': 63, 'g': 64, 'h': 65, 'i': 66, 'j': 67, 'k': 68, 'l': 69, 'm': 70, 'n': 71, 'o': 72, 'p': 73, 'q': 74, 'r': 75, 's': 76, 't': 77, 'u': 78, 'v': 79, 'w': 80, 'x': 81, 'y': 82, 'z': 83, 'À': 84, 'Æ': 85, 'Ç': 86, 'É': 87, 'à': 88, 'â': 89, 'æ': 90, 'ç': 91, 'è': 92, 'é': 93, 'ê': 94, 'ë': 95, 'î': 96, 'œ': 97, '—': 98, '‘': 99, '’': 100, '“': 101, '”': 102, '•': 103, '…': 104, '™': 105, '\\ufeff': 106}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id_to_char = {idx: char for idx, char in enumerate(ordered_unique_chars)}"
      ],
      "metadata": {
        "id": "1aaznpkweX5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_ids = [char_to_id[char] for char in content]\n",
        "print(content_ids[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iDSg4NGej0s",
        "outputId": "b76ca97c-5272-4b0e-b219-57d1fca287f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[48, 65, 62, 2, 44, 75, 72, 67, 62, 60, 77, 2, 35, 78, 77, 62, 71, 59, 62, 75, 64, 2, 62, 30, 72, 72, 68, 2, 72, 63]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(content_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsVCoPJehqwC",
        "outputId": "56e7925f-887d-4f27-e7bc-f8bb491c58fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5378662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length=50\n",
        "\n",
        "def chunk_list(my_list, chunk_size):\n",
        "    chunks = []\n",
        "    for i in range(0, len(my_list), chunk_size):\n",
        "        chunks.append(my_list[i:i + chunk_size])\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_list(content_ids, sequence_length+1)\n",
        "\n",
        "print(chunks[-1])\n",
        "del chunks[-1]# Equal length chunks\n",
        "print(chunks[-1])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ial91fKFj4Yn",
        "outputId": "ad1c8734-523d-4f22-d0d9-b9db9d5ef1b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 72, 78, 75, 2, 62, 70, 58, 66, 69, 2, 71, 62, 80, 76, 69, 62, 77, 77, 62, 75, 2, 77, 72, 2, 65, 62, 58, 75, 2, 58, 59, 72, 78, 77, 2, 71, 62, 80, 2, 62, 30, 72, 72, 68, 76, 14, 1, 1]\n",
            "[62, 69, 73, 2, 73, 75, 72, 61, 78, 60, 62, 2, 72, 78, 75, 2, 71, 62, 80, 2, 62, 30, 72, 72, 68, 76, 12, 2, 58, 71, 61, 2, 65, 72, 80, 2, 77, 72, 1, 76, 78, 59, 76, 60, 75, 66, 59, 62, 2, 77, 72]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [sublist[:-1] for sublist in chunks]\n",
        "targets = [sublist[-1] for sublist in chunks]\n",
        "print(inputs[:2])\n",
        "print(targets[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItsGR-BxuiZ8",
        "outputId": "cfcfeab5-a9ff-45a7-b0ff-5de121d0fd9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[48, 65, 62, 2, 44, 75, 72, 67, 62, 60, 77, 2, 35, 78, 77, 62, 71, 59, 62, 75, 64, 2, 62, 30, 72, 72, 68, 2, 72, 63, 2, 48, 65, 62, 2, 31, 72, 70, 73, 69, 62, 77, 62, 2, 51, 72, 75, 68, 76, 2], [63, 2, 51, 66, 69, 69, 66, 58, 70, 2, 47, 65, 58, 68, 62, 76, 73, 62, 58, 75, 62, 1, 2, 2, 2, 2, 1, 48, 65, 66, 76, 2, 62, 59, 72, 72, 68, 2, 66, 76, 2, 63, 72, 75, 2, 77, 65, 62, 2, 78]]\n",
            "[72, 76]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(inputs))\n",
        "print(len(targets))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYGwFs9Fy8XZ",
        "outputId": "5af33f41-ba08-42bd-9e4e-9c802b318f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105463\n",
            "105463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "data_pairs = list(zip(inputs, targets))\n",
        "# random.shuffle(data_pairs)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YbBxddd82IKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batches(data_pairs, batch_size):\n",
        "    return [data_pairs[i:i + batch_size] for i in range(0, len(data_pairs), batch_size)]\n"
      ],
      "metadata": {
        "id": "NdVzIpsv6NWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "# batches = create_batches(data_pairs, batch_size)\n",
        "# print(len(batches[-1]))\n",
        "# del batches[-1]\n",
        "# print(len(batches[-1]))\n"
      ],
      "metadata": {
        "id": "VKdrhxE56O5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_inputs = [sublist[:-1] for sublist in chunks]\n",
        "# batch_targets = [sublist[-1] for sublist in chunks]"
      ],
      "metadata": {
        "id": "T7M9UdCB6nl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for batch in batches:\n",
        "#     batch_inputs = [pair[0] for pair in batch]\n",
        "#     batch_targets = [pair[1] for pair in batch]\n",
        "#     batch_inputs = torch.tensor(batch_inputs, dtype=torch.long)\n",
        "#     batch_targets = torch.tensor(batch_targets, dtype=torch.long)"
      ],
      "metadata": {
        "id": "uRk8gs4U7h1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "\n",
        "        assert embedding_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embedding_dim // num_heads\n",
        "\n",
        "        # Linear layers for queries, keys, and values\n",
        "        self.query = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.key = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.value = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "        # Output linear layer\n",
        "        self.out = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, embedding_dim = x.size()\n",
        "\n",
        "        # Linear projections\n",
        "        Q = self.query(x)  # Shape: [batch_size, seq_length, embedding_dim]\n",
        "        K = self.key(x)    # Shape: [batch_size, seq_length, embedding_dim]\n",
        "        V = self.value(x)  # Shape: [batch_size, seq_length, embedding_dim]\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        Q = Q.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        # Now Q, K, V have shape: [batch_size, num_heads, seq_length, head_dim]\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        # Scores shape: [batch_size, num_heads, seq_length, seq_length]\n",
        "\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        # Attention weights shape: [batch_size, num_heads, seq_length, seq_length]\n",
        "\n",
        "        attention_output = torch.matmul(attention_weights, V)\n",
        "        # Attention output shape: [batch_size, num_heads, seq_length, head_dim]\n",
        "\n",
        "        # Concatenate heads\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_length, embedding_dim)\n",
        "        # Attention output shape after concatenation: [batch_size, seq_length, embedding_dim]\n",
        "\n",
        "        # Final linear layer\n",
        "        output = self.out(attention_output)\n",
        "        # Output shape: [batch_size, seq_length, embedding_dim]\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "JXIwE5Gqk4XV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embedding_dim, feedforward_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(embedding_dim, feedforward_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(feedforward_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "abwailCE3-_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, feedforward_dim):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.attention = MultiHeadSelfAttention(embedding_dim, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
        "        self.feed_forward = FeedForward(embedding_dim, feedforward_dim)\n",
        "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Self-attention with residual connection and layer norm\n",
        "        attn_output = self.attention(x)\n",
        "        x = x + attn_output  # Residual connection\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # Feed-forward network with residual connection and layer norm\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + ff_output  # Residual connection\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "wbQtLdCH4xCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class SimpleGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, num_layers, feedforward_dim, max_seq_length):\n",
        "        super(SimpleGPT, self).__init__()\n",
        "\n",
        "        # Token and positional embeddings\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.position_embedding = nn.Embedding(max_seq_length, embedding_dim)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(embedding_dim, num_heads, feedforward_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "        # Max sequence length\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length = x.size()\n",
        "\n",
        "        # Generate position indices and get positional embeddings\n",
        "        positions = torch.arange(0, seq_length, device=x.device).unsqueeze(0).expand(batch_size, seq_length)\n",
        "        token_embeddings = self.token_embedding(x)\n",
        "        position_embeddings = self.position_embedding(positions)\n",
        "\n",
        "        # Combine token and positional embeddings\n",
        "        x = token_embeddings + position_embeddings\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Output layer\n",
        "        logits = self.output_layer(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FmdOCcdgpL5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Hyperparameters\n",
        "vocab_size = len(char_to_id)\n",
        "embedding_dim = 128\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "feedforward_dim = 256\n",
        "max_seq_length = sequence_length  # Should match the sequence length used in data preparation\n",
        "\n",
        "# Initialize the model\n",
        "model = SimpleGPT(vocab_size, embedding_dim, num_heads, num_layers, feedforward_dim, max_seq_length)"
      ],
      "metadata": {
        "id": "Pgos1Y4k3xpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "OT359nuy7zwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "# Move the model to the appropriate device (CPU or GPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    # Shuffle the data at the beginning of each epoch\n",
        "    random.shuffle(data_pairs)\n",
        "\n",
        "    # Re-create batches from the shuffled data\n",
        "    batches = create_batches(data_pairs, batch_size)\n",
        "\n",
        "\n",
        "    if len(batches[-1]) < batch_size:\n",
        "        del batches[-1]\n",
        "\n",
        "    for batch in batches:\n",
        "        # Extract inputs and targets from the batch\n",
        "        batch_inputs = [pair[0] for pair in batch]\n",
        "        batch_targets = [pair[1] for pair in batch]\n",
        "\n",
        "        # Convert lists to tensors and move to device\n",
        "        batch_inputs = torch.tensor(batch_inputs, dtype=torch.long).to(device)\n",
        "        batch_targets = torch.tensor(batch_targets, dtype=torch.long).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_inputs)  # outputs shape: [batch_size, seq_length, vocab_size]\n",
        "\n",
        "        # Get logits for the last time step\n",
        "        outputs = outputs[:, -1, :]  # Shape: [batch_size, vocab_size]\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, batch_targets)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    average_loss = total_loss / len(batches)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4HIwI9zBFgb",
        "outputId": "eead3f23-aaa3-46d7-e30e-757ac3778a40"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 1.1712\n",
            "Epoch 2/20, Loss: 1.1809\n",
            "Epoch 3/20, Loss: 1.1763\n",
            "Epoch 4/20, Loss: 1.1681\n",
            "Epoch 5/20, Loss: 1.1598\n",
            "Epoch 6/20, Loss: 1.1614\n",
            "Epoch 7/20, Loss: 1.1658\n",
            "Epoch 8/20, Loss: 1.1573\n",
            "Epoch 9/20, Loss: 1.1609\n",
            "Epoch 10/20, Loss: 1.1550\n",
            "Epoch 11/20, Loss: 1.1622\n",
            "Epoch 12/20, Loss: 1.1580\n",
            "Epoch 13/20, Loss: 1.1508\n",
            "Epoch 14/20, Loss: 1.1493\n",
            "Epoch 15/20, Loss: 1.1559\n",
            "Epoch 16/20, Loss: 1.1459\n",
            "Epoch 17/20, Loss: 1.1404\n",
            "Epoch 18/20, Loss: 1.1497\n",
            "Epoch 19/20, Loss: 1.1458\n",
            "Epoch 20/20, Loss: 1.1376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def generate_text(model, start_text, generate_length):\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Convert the starting text to token IDs\n",
        "    input_ids = [char_to_id.get(char, char_to_id[' ']) for char in start_text]\n",
        "    input_ids = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
        "\n",
        "    generated_text = start_text\n",
        "\n",
        "    for _ in range(generate_length):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs[:, -1, :]  # Get the logits for the last time step\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "        # Append the predicted character to the generated text\n",
        "        generated_text += id_to_char[next_id]\n",
        "\n",
        "        # Update the input_ids by appending the predicted character\n",
        "        next_id_tensor = torch.tensor([[next_id]], dtype=torch.long).to(device)\n",
        "        input_ids = torch.cat([input_ids, next_id_tensor], dim=1)\n",
        "        input_ids = input_ids[:, -model.max_seq_length:]  # Keep the sequence length consistent\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "MVt_mjysKdZw"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "start_text = \"Thy beauty shall no more be found,\"\n",
        "generate_length = 300  # Number of characters to generate\n",
        "\n",
        "generated = generate_text(model, start_text, generate_length)\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8x89QSHKmYv",
        "outputId": "d09c86c9-0977-4331-ea0f-db5fa973d997"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thy beauty shall no more be found,\n",
            "eriiart\n",
            "Ibat ddince, pats the brother both dear?\n",
            "\n",
            "PALETH, and Clarury than another Can at;\n",
            "\n",
            "ANTONY.\n",
            "Master France vales. What it it Is a brect, give high-unto the like my lick\n",
            "end I wo, I stone’er changels and to do combouches, as world\n",
            "Sny to these return. Will we this dear?\n",
            "\n",
            "OLIVIA.\n",
            "The comit he \n"
          ]
        }
      ]
    }
  ]
}